{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZSGYoq8pYPp",
        "outputId": "0a62cf2f-0ba0-4158-fde4-ba1693f1bb4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/Drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/Drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qICg3AGGrrJQ"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o5yz6lcB5MT8"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Loading data from a CSV file\n",
        "df = pd.read_csv('/content/Drive/MyDrive/NLP-LLM/books_1.Best_Books_Ever.csv')\n",
        "\n",
        "print(df.info())\n",
        "\n",
        "# Output of the first 5 rows of the table\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mdMUds67ANbQ"
      },
      "outputs": [],
      "source": [
        "!pip install vaderSentiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gfBNu1qFLZOP",
        "outputId": "94113ba9-d0cb-4385-ceef-976011f7edf2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the path to your dataset: /content/Drive/MyDrive/NLP-LLM/books_1.Best_Books_Ever.csv\n",
            "Enter User ID: 4728\n",
            "Processing epoch 0\n",
            "Processing epoch 1\n",
            "Processing epoch 2\n",
            "Processing epoch 3\n",
            "Processing epoch 4\n",
            "Processing epoch 5\n",
            "Processing epoch 6\n",
            "Processing epoch 7\n",
            "Processing epoch 8\n",
            "Processing epoch 9\n",
            "Processing epoch 10\n",
            "Processing epoch 11\n",
            "Processing epoch 12\n",
            "Processing epoch 13\n",
            "Processing epoch 14\n",
            "Processing epoch 15\n",
            "Processing epoch 16\n",
            "Processing epoch 17\n",
            "Processing epoch 18\n",
            "Processing epoch 19\n",
            "Top Recommended Books:\n",
            "                                   bookId  Estimated Rating  \\\n",
            "0                     15852670-ode-to-uke          4.313895   \n",
            "1                   5206311-rovella-starr          4.298820   \n",
            "2                   220741.First_and_Only          4.290042   \n",
            "3                      465904.Dragonsdawn          4.287323   \n",
            "4  31700489-el-laberinto-de-los-esp-ritus          4.284133   \n",
            "5               2823.The_Birth_of_Tragedy          4.280306   \n",
            "6         4506536-twenty-years-in-siberia          4.278139   \n",
            "7          18042290-the-best-scandal-ever          4.277752   \n",
            "8         51926728-the-complete-anthology          4.273562   \n",
            "9                     18160436-the-letter          4.271735   \n",
            "\n",
            "                                       title  \n",
            "0                                 Ode to Uke  \n",
            "1        Rovella Starr: A Love-Starved Bitch  \n",
            "2                             First and Only  \n",
            "3                                Dragonsdawn  \n",
            "4              El laberinto de los esp√≠ritus  \n",
            "5                       The Birth of Tragedy  \n",
            "6                    Twenty Years in Siberia  \n",
            "7                      The Best Scandal Ever  \n",
            "8  The Complete Anthology: John Lars Zwerenz  \n",
            "9                                 The Letter  \n",
            "\n",
            "Evaluation Metrics:\n",
            "RMSE (Random Forest): 0.2545080749719133\n",
            "MAE (Random Forest): 0.2545080749719133\n",
            "RMSE: 0.3695\n",
            "RMSE: 0.3679\n",
            "RMSE: 0.3629\n",
            "RMSE: 0.3688\n",
            "RMSE: 0.3663\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from surprise import SVD, Dataset, Reader, accuracy\n",
        "from surprise.model_selection import train_test_split as surprise_train_test_split\n",
        "from scipy.sparse import hstack\n",
        "from textblob import TextBlob\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "import logging\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "def load_data(file_path):\n",
        "    \"\"\"Load data from a CSV file.\"\"\"\n",
        "    df = pd.read_csv(file_path)\n",
        "    df['description'].fillna('', inplace=True)  # Ensuring no null values\n",
        "    df['bookFormat'].fillna('Unknown', inplace=True)  # Fill missing formats with 'Unknown'\n",
        "    return df\n",
        "\n",
        "def add_sentiment_features(df):\n",
        "    \"\"\"Extract sentiment from descriptions using VADER.\"\"\"\n",
        "    analyzer = SentimentIntensityAnalyzer()\n",
        "    df['sentiment'] = df['description'].apply(lambda x: analyzer.polarity_scores(x)['compound'])\n",
        "    return df\n",
        "\n",
        "def preprocess_text_data(text_series, max_features=100):\n",
        "    \"\"\"Preprocess text data using TF-IDF vectorization with bi-grams.\"\"\"\n",
        "    vectorizer = TfidfVectorizer(stop_words='english', max_features=max_features, ngram_range=(1, 2))\n",
        "    return vectorizer.fit_transform(text_series)\n",
        "\n",
        "def encode_features(df):\n",
        "    \"\"\"Encode features including categorical and numerical data.\"\"\"\n",
        "    df = add_sentiment_features(df)\n",
        "    tfidf_features = preprocess_text_data(df['description'])\n",
        "\n",
        "    encoder = OneHotEncoder(handle_unknown='ignore')\n",
        "    categorical_features = encoder.fit_transform(df[['bookFormat']])\n",
        "\n",
        "    numerical_pipeline = Pipeline([\n",
        "        ('imputer', SimpleImputer(strategy='median')),\n",
        "        ('scaler', StandardScaler())\n",
        "    ])\n",
        "    numerical_features = numerical_pipeline.fit_transform(df[['sentiment']])\n",
        "\n",
        "    X = hstack([tfidf_features, categorical_features, numerical_features])\n",
        "    return X, df['rating'].values\n",
        "\n",
        "def train_model(X, y):\n",
        "    \"\"\"Train a RandomForest model.\"\"\"\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "    model = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    y_pred = model.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    rmse = mean_absolute_error(y_test, y_pred)\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "\n",
        "    return model, X_test, y_test, y_pred, mse, rmse, mae\n",
        "\n",
        "def train_collaborative_filtering(df):\n",
        "    \"\"\"Train an SVD model for collaborative filtering.\"\"\"\n",
        "    reader = Reader(rating_scale=(1, 10))\n",
        "    data = Dataset.load_from_df(df[['userID', 'bookId', 'rating']], reader)\n",
        "    trainset, testset = surprise_train_test_split(data, test_size=0.2)\n",
        "    svd_model = SVD(n_factors=50, n_epochs=20, verbose=True)\n",
        "    svd_model.fit(trainset)\n",
        "    return svd_model, testset\n",
        "\n",
        "def get_recommendations(df, svd_model, user_id, top_n=10):\n",
        "    \"\"\"Generate book recommendations for a given user.\"\"\"\n",
        "    unique_books = df['bookId'].unique()\n",
        "    user_books = df[df['userID'] == user_id]['bookId'].unique()\n",
        "    books_to_predict = np.setdiff1d(unique_books, user_books)\n",
        "\n",
        "    predictions = [(book, svd_model.predict(user_id, book).est) for book in books_to_predict]\n",
        "    recommendations = sorted(predictions, key=lambda x: x[1], reverse=True)[:top_n]\n",
        "    recommended_books = pd.DataFrame(recommendations, columns=['bookId', 'Estimated Rating'])\n",
        "    recommended_books = pd.merge(recommended_books, df[['bookId', 'title']], on='bookId').drop_duplicates('bookId')\n",
        "    return recommended_books\n",
        "\n",
        "def main(file_path, user_id):\n",
        "    \"\"\"Main function to run the recommendation system.\"\"\"\n",
        "    df = load_data(file_path)\n",
        "    X, y = encode_features(df)\n",
        "    item_model, X_test, y_test, y_pred, mse, rmse, mae = train_model(X, y)\n",
        "    svd_model, testset = train_collaborative_filtering(df)\n",
        "\n",
        "    # Get book recommendations\n",
        "    recommendations = get_recommendations(df, svd_model, user_id)\n",
        "    print(\"Top Recommended Books:\")\n",
        "    print(recommendations)\n",
        "\n",
        "    # Evaluate model\n",
        "    print(\"\\nEvaluation Metrics:\")\n",
        "    print(f\"RMSE (Random Forest): {rmse}\")\n",
        "    print(f\"MAE (Random Forest): {mae}\")\n",
        "    run_kfold_cross_validation(df)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    file_path = input(\"Enter the path to your dataset: \")\n",
        "    user_id = int(input(\"Enter User ID: \"))\n",
        "    main(file_path, user_id)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}